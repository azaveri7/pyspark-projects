1. Course Link :

   https://github.com/LearningJournal/Spark-Programming-In-Python

2. To execute Spark programs

   - Interactive clients
     spark-shell, notebook

   - Submit job
     spark-submit, databricks, notebook, rest api

3. When we do groupBy in a dataframe, there will be shuffle sort of data in the available partitions

  The number of partitions in which data will be shuffled is not fixed, so to control this behaviour,
  set property

  spark.sql.shuffle.partitions = 2

  So after groupBy operation, there will be only 2 partitions.

4. When to use RDD.

     In case if we need to perform an operation on each row of spark dataset, we need to use RDD map method.
     For e.g. replace '(single quote) with "(double quote) in each row.

5. When u want to use spark SQL use createOrReplaceTempView on spark data-frame.

6. When u read csv and json files in dataframe reader, the date type comes as string in the dataframe.
   But if we read parquet, the data field will have type date. So parquet file contains data type alongwith
   field name. So parquet is preferred and default file format with spark.

7. To support AVRO, we need to add scala / java dependencies.
   Python does not support this.

   So we need to add this in spark-defaults.conf

   spark.jars.packages org.apache.spark:spark-avro_2.11:2.4.5
