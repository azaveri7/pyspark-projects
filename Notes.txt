1. Course Link :

   https://github.com/LearningJournal/Spark-Programming-In-Python

2. To execute Spark programs

   - Interactive clients
     spark-shell, notebook

   - Submit job
     spark-submit, databricks, notebook, rest api

3. When we do groupBy in a dataframe, there will be shuffle sort of data in the available partitions

  The number of partitions in which data will be shuffled is not fixed, so to control this behaviour,
  set property

  spark.sql.shuffle.partitions = 2

  So after groupBy operation, there will be only 2 partitions.

4. When to use RDD.

     In case if we need to perform an operation on each row of spark dataset, we need to use RDD map method.
     For e.g. replace '(single quote) with "(double quote) in each row.

5. When u want to use spark SQL use createOrReplaceTempView on spark data-frame.

6. When u read csv and json files in dataframe reader, the date type comes as string in the dataframe.
   But if we read parquet, the data field will have type date. So parquet file contains data type alongwith
   field name. So parquet is preferred and default file format with spark.

7. To support AVRO, we need to add scala / java dependencies.
   Python does not support this.

   So we need to add this in spark-defaults.conf

   spark.jars.packages org.apache.spark:spark-avro_2.11:2.4.5
   spark.driver.extraJavaOptions      -Dlog4j.configuration=file:log4j.properties -Dspark.yarn.app.container.log.dir=app-logs -Dlogfile.name=hello-spark


8. To check id of paritions and number of records in each partition

  logger.info("Number of partitions : " + str(flightTimeParquetDF.rdd.getNumPartitions()))
  flightTimeParquetDF.groupby(spark_partition_id()).count().show()

9. To divide output into multiple Partitions

partitionDF = flightTimeParquetDF.repartition(5)
 logger.info("Number of partitions earlier : " + str(partitionDF.rdd.getNumPartitions()))
 partitionDF.groupby(spark_partition_id()).count().show()

 partitionDF.write \
     .format("avro") \
     .mode("overwrite") \
     .option("path", "dataSink/partitionDF/avro/") \
     .save()

10. To partition by particular columns

  flightTimeParquetDF.write \
      .format("json") \
      .mode("overwrite") \
      .option("path", "dataSink/json/") \
      .partitionBy("OP_CARRIER", "ORIGIN") \
      .save()

    In the output files, columns OP_CARRIER and ORIGIN will be missing.

11. To limit the records per file in a partition.

flightTimeParquetDF.write \
    .format("json") \
    .mode("overwrite") \
    .option("path", "dataSink/json/") \
    .partitionBy("OP_CARRIER", "ORIGIN") \
    .option("maxRecordsPerFile", "10000") \
    .save()

12. If partitionBy is used on a column having large number of unique values,
    it will create a lot of partitions.

    flightTimeParquetDF.write \
        .mode("overwrite") \
        .partitionBy("ORIGIN","OP_CARRIER") \
        .saveAsTable("flight_data_tbl")

    Replace it with:

    flightTimeParquetDF.write \
        .mode("overwrite") \
        .bucketBy(5, "ORIGIN","OP_CARRIER") \
        .saveAsTable("flight_data_tbl")

13. UDF function:

survey_df2 = survye_df.withColumn("Gender", parse_gender(gender))

This will not work, directly a function cannot be applied to a df.

we need to register UDF with spark driver by using

parse_gender_udf = udf(parse_gender(), returnType=StringType)
logger.info("Catalog Entry:")
[logger.info(r) for r in spark.catalog.listFunctions() if "parse_gender" in r.name]

Once registered, the function will be sent to all executors so that they can execute it.

This line will not make UDF entry into Spark catalog:
parse_gender_udf = udf(parse_gender(), returnType=StringType)

So to make an entry into Spark catalog, use this:
spark.udf.register("parse_gender_udf", parse_gender, StringType())

So final code snippet is:

    parse_gender_udf = udf(parse_gender(), returnType=StringType)
    logger.info("Catalog Entry:")
    [logger.info(r) for r in spark.catalog.listFunctions() if "parse_gender" in r.name]

    survey_df2 = survey_df.withColumn("Gender", parse_gender_udf("Gender"))
    survey_df2.show(10)

    spark.udf.register("parse_gender_udf", parse_gender, StringType())
    logger.info("Catalog Entry:")
    [logger.info(r) for r in spark.catalog.listFunctions() if "parse_gender" in r.name]

    survey_df3 = survey_df.withColumn("Gender", expr("parse_gender_udf(Gender)"))
    survey_df3.show(10)

14. Aggregations example

first way:

invoice_df.createOrReplaceTempView("sales")
    summary_sql = spark.sql("""
              SELECT Country, InvoiceNo,
                    sum(Quantity) as TotalQuantity,
                    round(sum(Quantity*UnitPrice),2) as InvoiceValue
              FROM sales
              GROUP BY Country, InvoiceNo""")

    summary_sql.show()

second way:

    summary_df = invoice_df \
        .groupBy("Country", "InvoiceNo") \
        .agg(f.sum("Quantity").alias("TotalQuantity"),
             f.round(f.sum(f.expr("Quantity * UnitPrice")), 2).alias("InvoiceValue"),
             f.expr("round(sum(Quantity * UnitPrice),2) as InvoiceValueExpr")
             )

    summary_df.show()

agg() -> function is specifically designed to accept a list of agg functions.
